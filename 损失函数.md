[机器学习中的 7 大*损失函数*实战总结（附Python演练）](https://zhuanlan.zhihu.com/p/80370381)

本文主要介绍下深度学习(图像领域)几个常用loss，不仅搞明白其原理，还顺便借助pytorch的源码看看其实现。主要包括：

- **CrossEntropyLoss（交叉熵损失）**
- **NLLLoss（负对数似然损失）**
- **Binary Cross Entropy（二元交叉熵损失/BCELoss）**
- **l1 loss（平均绝对损失/MAE Loss）**
- **smooth l1 loss（平滑l1损失）**
- **l2 loss（均方损失/MSE Loss）**


总体来说，有了之前文章：[【深度学习理论】一文搞透Dropout、L1L2正则化/权重衰减](https://zhuanlan.zhihu.com/p/147029320) 关于l1-norm和l2-norm的介绍，对于l1/l2 loss都是相对容易理解的。**所以本文的重点在于——交叉熵CrossEntropy损失函数。**为什么在图像分类/检测等领域，交叉熵loss被应用的如此广泛？ 其和sigmoid、softmax又怎样的联系？且看本文分解：）

**![img](https://pic3.zhimg.com/v2-00493889e4162590c3da320e6945db6a_b.jpg)**



## 1.Sigmoid vs Softmax

为什么要把这两个东西放在一起讲？因为很多深度学习初学者(包括我)可能会偶尔将两者弄混淆，傻傻分不清楚(名称or使用方式)，而这二者又常常和交叉熵损失函数有着紧密的联系，所以我们今天就来研究下。
 
首先，在之前的文章中——[【吴恩达深度学习】—神经网络中的激活函数](https://zhuanlan.zhihu.com/p/76048966)我们介绍过sigmoid。**sigmoid函数也称为Logistic函数。它常用在机器学习/深度学习中作为激活函数，用于把输入值激活后归一化到0~1的范围(激活后的输出呈伯努利分布)。通常，其多用于神经网络模型中间层做激活函数**，当然，在relu风靡之后，用sigmoid激活的情况就少了很多（简单来说，因为relu激活更快更好）。除此之外，**sigmoid还可以用在最后的输出层/全连接层，适用于类别非互斥的多分类问题中，**其输出的概率和不一定==1（譬如一个多标签分类任务，一个小男孩既可以分类为boy，也可以分类为human，二者可以同时存在），通常配合交叉熵损失以计算多分类损失。

**而softmax作为激活函数，通常只用在最后的输出层/全连接层，在输出层后接上softmax激活得到多项分布的输出值(0~1范围)，常用在类别独立的多分类任务中**(譬如在一个手写数字识别的模型中，将0~9共10个类别数的输出用softmax处理后，10个输出的概率和为1)。
 
本质上，sigmoid和softmax仅仅是两种不同的公式而已，用于对输入进行“激活”，只要你愿意可以在任意地方使用。当然，本文的重点是在二者和交叉熵损失的关系上面，即使用sigmoid/softmax用于输出层的激活。为了方便后面介绍交叉熵，我们首先需要理解sigmoid/softmax，因为**通常需要用softmax/sigmoid函数处理后，才能输入交叉熵函数进行损失的计算。**

## 1.1 公式定义

### sigmoid

![[公式]](https://www.zhihu.com/equation?tex=F%28X_i%29%3D%5Cfrac%7B1%7D%7B1%2B%5Cexp%28-X_i%29%7D%3D%5Cfrac%7B%5Cexp%28X_i%29%7D%7B%5Cexp%28X_i%29%2B1%7D) 
**图像：**

![img](https://pic4.zhimg.com/v2-42ecafd86d4bbdc71af4b04187e7e56b_b.jpg)图片：李宏毅《一天搞懂深度学习》


**softmax**

![[公式]](https://www.zhihu.com/equation?tex=F%28X_i%29%3D%5Cfrac%7B%5Cexp%28X_i%29%7D%7B%5Csum_%7Bj%3D0%7D%5Ek%5Cexp%28X_j%29%7D%2C+i%3D0%2C+1%2C+%5Cldots%2C+k) 

## 1.2 二分类/多分类和多标签分类

这里首先需要明确，分类问题根据是否互斥，可以大致分为：多分类和多标签分类(非互斥多分类)；而二分类即分类类别数为2，是一种多分类的特例(如硬币正/反面；天气下雨/不下雨)。所以总体来说，**分类问题主要有下面三种：**

- **二分类**
- **多分类**
- **非互斥多分类/多标签分类**


多分类是指类别互斥的分类，一个输入只能是多个分类中的一种。如：一个筛子的6种点数；猫、狗、鸡、鸭等。

二分类是多分类的一种特例，即类别数为2。如硬币的正、反；天气的晴天、阴雨等。

而非互斥的多分类/多标签分类是指，分类的类别直接不互斥，可以叠加。譬如：输入的图像即包含猫，也包含狗，还有猴子，就可以打上cat,dog,mokey的标签；或者一个输入，即是猫，也是哺乳动物。
 
**通常来说，softmax用于多分类；sigmoid函数用于非互斥多分类/多标签分类；而二分类，通常二者皆可使用，**只是使用方式上有所差异。

### 多分类/多标签分类

**sigmoid**
**sigmoid仅对输入做了归一化处理，使得任意输入都能产生0~1的输出，适合多标签分类(类别间不独立的任务)，各个类别之间的输出是独立的(不会互相影响)，但其输出的概率和并不一定==1，所有并不适合多分类任务。**
例：多标签分类任务 猫，猫科，狗。
输出值：[3,1,-3]，经sigmoid激活后的输出：[1/1+1.3956, 1/1+2.7183, 1/1+20.0855] = [0.4174, 0.2689,0.0474] 表明此目标为猫、猫科动物、狗的概率分别为41.74%、26.89%、4.74%。


**softmax**
**而softmax不仅对输入起到归一化作用，且压缩了输出分布，使得整体输入的概率和为1，适合类别独立的多分类任务。**
例：类别独立(互斥）的多分类任务：猫，人，狗。
这些类别都是互相独立的，不可能既是猫又是狗，或者同时是人或狗：）。
我们看下图示的softmax计算过程：

![img](https://pic3.zhimg.com/v2-2dc68a680e3ca3d7a5907d965edb2ab2_b.jpg)图片：李宏毅《一天搞懂深度学习》


输出经过softmax处理后的概率分别为0.88,0.12,0，整体概率和为100%
**正因为使用softmax后，其输出的概率和为1，故softmax适用于类别独立的多分类任务；同样，sigmoid的输出类别概率则互不干涉，没有加和 = 1的限制，适用于类别不互斥的多分类任务。**

### 二分类

刚刚前面说了，softmax适用于多分类而sigmoid适用于非互斥多分类/多标签分类。那么具体到二分类问题，用哪个呢 ？二分类是多分类的特殊形式，这里的二分类通常是指互斥的二分类。在知乎：[二分类问题，应该选择sigmoid还是softmax？](https://www.zhihu.com/question/295247085/answer/711229736)描述了这个问题，**从公式推导的角度来看，二者并无差异，用哪个都ok**。我这边就直接引用了：
 
如果使用sigmoid的话，输出层有一个结点，值为 ![[公式]](https://www.zhihu.com/equation?tex=%5Ctheta_h) 。然后类别1，2的预测概率分别为

![img](https://pic4.zhimg.com/v2-833fd9326594dda48b1c1bd466487b63_b.jpg)

![img](https://pic3.zhimg.com/v2-d1b7cf074a04d7a54a80b0cbe9f0d9c6_b.jpg)


而如果使用softmax，输出层有两个结点，值分别是 ![[公式]](https://www.zhihu.com/equation?tex=%5Ctheta_1h%EF%BC%8C%5Ctheta_2h)   。然后类别1，2的预测概率分别为

![img](https://pic3.zhimg.com/v2-d1b7cf074a04d7a54a80b0cbe9f0d9c6_b.jpg)

可以看到，sigmoid网络中的 ![[公式]](https://www.zhihu.com/equation?tex=%5Ctheta) 与softmax网络中的![[公式]](https://www.zhihu.com/equation?tex=%5Ctheta_1+-+%5Ctheta_2)是等价的。也就是说，不管sigmoid网络能产生什么样的预测，也一定存在softmax网络能产生相同的预测，只要令 ![[公式]](https://www.zhihu.com/equation?tex=%5Ctheta_1+-+%5Ctheta_2) 即可；反之亦然。

而softmax网络的训练过程可以看作是在直接优化 ![[公式]](https://www.zhihu.com/equation?tex=%5Ctheta_1+-+%5Ctheta_2) ，优化结果和sigmoid应该没什么差异。所以我自己在做的时候会直接用softmax，这样也比较方便改成多分类的模型。

> **有一点需要注意，通常使用sigmoid进行二分类时，输出仅用一个神经元即可；而使用softmax二分类时，通常用2个神经元分别表示2个分类的输出。**

## 2.交叉熵

交叉熵，常用于图像分类问题中作为损失函数，当然分类问题也可以使用其他损失譬如focal loss，不过交叉熵使用的最为广泛。对于交叉熵损失，可能很多人一上来就直接用了，并没有深刻理解其含义，我也是～直觉上感觉“熵”是一个度量复杂度的单位(著名的熵增定律)，而交叉熵是神马？log表示的意义何在？

![img](https://pic1.zhimg.com/v2-b032e1ff624729b960df5fcf8a9bf9c0_b.jpg)


直到某一天，我看到了一篇文章——[一文搞懂交叉熵在机器学习中的使用，透彻理解交叉熵背后的直觉](https://link.zhihu.com/?target=https%3A//blog.csdn.net/tsyccnh/article/details/79163834) 后豁然开朗。
我觉得作者写的足够清晰易懂，so下面2.1节的部分内容我就直接从原文中copy过来了：）

## 2.1 信息论

交叉熵是信息论中的一个概念，要想了解交叉熵的本质，需要先从最基本的概念讲起。

### 信息量

首先是信息量。假设我们听到了两件事，分别如下：

- **事件A：巴西队进入了2018世界杯决赛圈。**
- **事件B：中国队进入了2018世界杯决赛圈。**


仅凭直觉来说，显而易见事件B的信息量比事件A的信息量要大。究其原因，是因为事件A发生的概率很大，事件B发生的概率很小。所以当越不可能的事件发生了，我们获取到的信息量就越大。越可能发生的事件发生了，我们获取到的信息量就越小。那么信息量应该和事件发生的概率有关。
 
假设 ![[公式]](https://www.zhihu.com/equation?tex=X) 是一个离散型随机变量，其取值集合为 ![[公式]](https://www.zhihu.com/equation?tex=%5Cchi) ,概率分布函数 ![[公式]](https://www.zhihu.com/equation?tex=p%28x%29%3DPr%28X%3Dx%29%2Cx%5Cin%5Cchi) 则定义事件 ![[公式]](https://www.zhihu.com/equation?tex=X%3Dx_0) 的信息量为：![[公式]](https://www.zhihu.com/equation?tex=I%28x_0%29%3D-log%28p%28x_0%29%29) 

由于是概率所以p(x0)p(x0)的取值范围是[0,1][0,1],绘制为图形如下：

![img](https://pic1.zhimg.com/v2-ad8a760ab8e7ea725a18da0271752e3c_b.png)


可见该函数符合我们对信息量的直觉

### 熵Entropy

考虑另一个问题，对于某个事件，有n种可能性，每一种可能性都有一个概率 ![[公式]](https://www.zhihu.com/equation?tex=p%28xi%29) 这样就可以计算出某一种可能性的信息量。举一个例子，假设你拿出了你的电脑，按下开关，会有三种可能性，下表列出了每一种可能的概率及其对应的信息量

![img](https://pic2.zhimg.com/v2-992d758c4af0b39f240125c103ecd46d_b.jpg)

>  注：文中的对数均为自然对数


我们现在有了信息量的定义，而熵用来表示所有信息量的期望，即：
 
 ![[公式]](https://www.zhihu.com/equation?tex=H%28X%29%3D-%5Csum_%7Bi%3D1%7D%5En+p%28x_i%29log%28p%28x_i%29%29) 
 
其中n代表所有的n种可能性，所以上面的问题结果就是
 
 ![[公式]](https://www.zhihu.com/equation?tex=H%28X%29%3D%E2%88%92%5Bp%28A%29log%28p%28A%29%29%2Bp%28B%29log%28p%28B%29%29%2Bp%28C%29%29log%28p%28C%29%29%5D) 

![[公式]](https://www.zhihu.com/equation?tex=%3D0.7%C3%970.36%2B0.2%C3%971.61%2B0.1%C3%972.30%3D0.804) 
 
然而有一类比较特殊的问题，比如投掷硬币只有两种可能，字朝上或花朝上。买彩票只有两种可能，中奖或不中奖。我们称之为0-1分布问题（二项分布的特例），对于这类问题，熵的计算方法可以简化为如下算式：
 
 ![[公式]](https://www.zhihu.com/equation?tex=%5Cbegin%7Beqnarray%7D+H%28X%29%26%3D%26-%5Csum_%7Bi%3D1%7D%5En+p%28x_i%29log%28p%28x_i%29%29%5C+%26%3D%26-p%28x%29log%28p%28x%29%29-%281-p%28x%29%29log%281-p%28x%29%29+%5Cend%7Beqnarray%7D) 
 

### 相对熵（KL散度）

相对熵又称KL散度,如果我们对于同一个随机变量 x 有两个单独的概率分布 P(x) 和 Q(x)，我们可以使用 KL 散度（Kullback-Leibler (KL) divergence）来衡量这两个分布的差异
 
维基百科对相对熵的定义

>  In the context of machine learning, DKL(P‖Q) is often called the information gain achieved if P is used instead of Q.
>  

即如果用P来描述目标问题，而不是用Q来描述目标问题，得到的信息增量。
 
在机器学习中，P往往用来表示样本的真实分布，比如[1,0,0]表示当前样本属于第一类。Q用来表示模型所预测的分布，比如[0.7,0.2,0.1]
直观的理解就是如果用P来描述样本，那么就非常完美。而用Q来描述样本，虽然可以大致描述，但是不是那么的完美，信息量不足，需要额外的一些“信息增量”才能达到和P一样完美的描述。如果我们的Q通过反复训练，也能完美的描述样本，那么就不再需要额外的“信息增量”，Q等价于P。
 
KL散度的计算公式：
 ![[公式]](https://www.zhihu.com/equation?tex=D_%7BKL%7D%28p%7C%7Cq%29%3D%5Csum_%7Bi%3D1%7D%5Enp%28x_i%29log%28%5Cfrac%7Bp%28x_i%29%7D%7Bq%28x_i%29%7D%29+%5Ctag%7B3.1%7D) 
 
n为事件的所有可能性。
 ![[公式]](https://www.zhihu.com/equation?tex=D_%7BKL%7D) 的值越小，表示q分布和p分布越接近

### 交叉熵

对式3.1变形可以得到：
 ![[公式]](https://www.zhihu.com/equation?tex=%5Cbegin%7Beqnarray%7D+D_%7BKL%7D%28p%7C%7Cq%29+%26%3D%26+%5Csum_%7Bi%3D1%7D%5Enp%28x_i%29log%28p%28x_i%29%29-%5Csum_%7Bi%3D1%7D%5Enp%28x_i%29log%28q%28x_i%29%29%5C+%26%3D%26+-H%28p%28x%29%29%2B%5B-%5Csum_%7Bi%3D1%7D%5Enp%28x_i%29log%28q%28x_i%29%29%5D+%5Cend%7Beqnarray%7D) 
 
等式的前一部分恰巧就是p的熵，等式的后一部分，就是交叉熵：
 
 ![[公式]](https://www.zhihu.com/equation?tex=H%28p%2Cq%29%3D-%5Csum_%7Bi%3D1%7D%5Enp%28x_i%29log%28q%28x_i%29%29) 
 
在机器学习中，我们需要评估label和predicts之间的差距，使用KL散度刚刚好，即 ![[公式]](https://www.zhihu.com/equation?tex=D_%7BKL%7D%28y%7C%7C%5Chat%7By%7D%29) ，由于KL散度中的前一部分 ![[公式]](https://www.zhihu.com/equation?tex=-H%28y%29) 不变，故在优化过程中，只需要关注交叉熵就可以了。所以一般在机器学习中直接用交叉熵做loss，评估模型。

## 2.2 交叉熵损失函数

由1.节中的描述，我们知道交叉熵的公式：
 ![[公式]](https://www.zhihu.com/equation?tex=H%28p%2Cq%29%3D-%5Csum_%7Bi%3D1%7D%5Enp%28x_i%29log%28q%28x_i%29%29) 
**深度学习中的交叉熵损失函数(cross entropy loss)，其源于信息论中对两个分布的偏离程度的度量，更具体点，来源于相对熵(KL散度)**。公式中，p为样本的真实分布，q为样本的预测分布，所以现在我们知道为什么对输出要进行log处理了～同时，此公式描述的交叉熵是通用的，不仅适合多分类问题，也适用于二分类问题。当二分类时，公式转换为：
 ![[公式]](https://www.zhihu.com/equation?tex=%5Cbegin%7Beqnarray%7D+H%28X%29%26%3D%26-%5Csum_%7Bi%3D1%7D%5En+p%28x_i%29log%28p%28x_i%29%29%5C+%26%3D%26-p%28x%29log%28q%28x%29%29-%281-p%28x%29%29log%281-q%28x%29%29+%5Cend%7Beqnarray%7D) 
 
在pytorch和tensorflow中的用来处理多分类/多标签分类的交叉熵损失函数如下：

### tensorflow(2.2.0)

- **多分类——softmax_cross_entropy_with_logits、sparse_softmax_cross_entropy_with_logits**
- **多标签分类——sigmoid_cross_entropy_with_logits**

### pytorch(1.5.0)

- **多分类——CrossEntropyLoss**
- **多标签分类——BCELoss、BCEWithLogitsLoss**

------

## 3. Pytorch常用损失函数

这里主要列举了几个pytorch实现的(torch.nn包)深度学习图像相关的常用损失函数:

- **1.负对数似然损失—NLLLoss**
- **2.交叉熵损失—CrossEntropyLoss**
- **3.二元交叉熵损失—BCELoss**
- **4.平均绝对损失—MAELoss/L1 Loss**
- **5.平滑L1损失—SmoothL1Loss**
- **6.均方误差损失—MSELoss/L2Loss**

## 3.1 交叉熵损失—CrossEntropyLoss


 ![[公式]](https://www.zhihu.com/equation?tex=%5Ctext%7Bloss%7D%28x%2C+class%29+%3D+-%5Clog%5Cleft%28%5Cfrac%7B%5Cexp%28x%5Bclass%5D%29%7D%7B%5Csum_j+%5Cexp%28x%5Bj%5D%29%7D%5Cright%29++++++++++++++++++++++++%3D+-x%5Bclass%5D+%2B+%5Clog%5Cleft%28%5Csum_j+%5Cexp%28x%5Bj%5D%29%5Cright%29) 
 
 ![[公式]](https://www.zhihu.com/equation?tex=%5Ctext%7Bloss%7D%28x%2C+class%29+%3D+weight%5Bclass%5D+%5Cleft%28-x%5Bclass%5D+%2B+%5Clog%5Cleft%28%5Csum_j+%5Cexp%28x%5Bj%5D%29%5Cright%29%5Cright%29) 
上面的公式是官方文档里给出的，看起来不太直观，我们看一下更易于辨认的交叉熵公式：
 
 ![[公式]](https://www.zhihu.com/equation?tex=-%5Csum_%7Bc%3D1%7D%5EMy_%7Bc%7D%5Clog%28p_%7Bc%7D%29) 

- M - 分类的类别总数 (dog, cat, fish...)
- log - 自然对数
- yc类别的label/gt 真实值(0或1)
- pc类别的预测值


也可以用预测和真实的分布来表示：
 ![[公式]](https://www.zhihu.com/equation?tex=H%28p%2Cq%29%3D-%5Csum_%7Bi%3D1%7D%5Enp%28x_i%29log%28q%28x_i%29%29) 

- p为真实类别分布
- q为预测类别分布

下面看一下pytorch的交叉熵实现：

```python
class CrossEntropyLoss(_WeightedLoss):
    __constants__ = ['ignore_index', 'reduction']

    def __init__(self, weight=None, size_average=None, ignore_index=-100,
                 reduce=None, reduction='mean'):
        super(CrossEntropyLoss, self).__init__(weight, size_average, reduce, reduction)
        self.ignore_index = ignore_index

    def forward(self, input, target):
        return F.cross_entropy(input, target, weight=self.weight,
                               ignore_index=self.ignore_index, reduction=self.reduction)
```

nn.CrossEntropyLoss()主要调用的是nn.functional.cross_entropy()方法

```python
def cross_entropy(input, target, weight=None, size_average=None, ignore_index=-100,
                  reduce=None, reduction='mean'):
    # type: (Tensor, Tensor, Optional[Tensor], Optional[bool], int, Optional[bool], str) -> Tensor
    if not torch.jit.is_scripting():
        tens_ops = (input, target)
        if any([type(t) is not Tensor for t in tens_ops]) and has_torch_function(tens_ops):
            print('if any([type(t) is not Tensor for t in tens_ops]) and has_torch_function(tens_ops) >>>>> True')
            return handle_torch_function(
                cross_entropy, tens_ops, input, target, weight=weight,
                size_average=size_average, ignore_index=ignore_index, reduce=reduce,
                reduction=reduction)
    if size_average is not None or reduce is not None:
        reduction = _Reduction.legacy_get_string(size_average, reduce)
    return nll_loss(log_softmax(input, 1), target, weight, None, ignore_index, None, reduction)
```

注意到，cross_entropy()的实现，关键在第14行的：***nll_loss(log_softmax(input, 1), target...)***
**即CrossEntropyLoss的实现，其实是调用了负对数似然损失—NLLLoss()。先将输入的预测input经log_softmax（）处理，然后再用nll_loss()计算input和target的交叉熵。**


这里**log_softmax()和nll_loss()的**底层实现都在c++代码中，具体以**nll_loss()的cpu实现为例，其方法具体实现是在**aten/src/ATen/native/LossNLL.cpp中的**nll_loss_out_frame()上。虽然c++代码不太好阅读，不过我们可以用torch.nn.Softmax()和torch.log()来模拟其功能。
 ![[公式]](https://www.zhihu.com/equation?tex=H%28p%2Cq%29%3D-%5Csum_%7Bi%3D1%7D%5Enp%28x_i%29log%28q%28x_i%29%29) 
简单来说log_softmax()的功能就是将input softmax化，然后取log对数；此步骤大致对应了交叉熵公式的右半部分；接着用负对数似然函数nll_loss()处理：nll_loss(log_softmax(input, 1), target...) 大致相当于将真实分类的 ![[公式]](https://www.zhihu.com/equation?tex=p%28x%29%C3%97log%28q%28x%29%29) 再累加取“负”号。
 
看个例子就明白了：

```python
import torch
import torch.nn.functional as F

input = torch.randn(2, 3, requires_grad=True)
print('===========    input    =========== \n', input)
softmax = torch.nn.Softmax(dim=1)
softmax_input = softmax(input)
print('=========== softmax_input=========== \n', softmax_input)
log_softmax = torch.log(softmax_input)
print('=========== log_softmax =========== \n', log_softmax)
target = torch.tensor([0, 1])
print('===========    target   =========== \n', target)
output = F.cross_entropy(input, target)
output.backward()
print('===========    output   =========== \n', output)
```

输出：

```python
===========    input    =========== 
 tensor([[ 1.4746,  0.7613, -0.5468],
        [ 1.1310,  0.2323,  0.5530]], requires_grad=True)
=========== softmax_input=========== 
 tensor([[0.6163, 0.3020, 0.0816],
        [0.5081, 0.2068, 0.2851]], grad_fn=<SoftmaxBackward>)
=========== log_softmax =========== 
 tensor([[-0.4840, -1.1972, -2.5053],
        [-0.6771, -1.5758, -1.2551]], grad_fn=<LogBackward>)
===========    target   =========== 
 tensor([0, 1])
torch.nn.functional nll_loss() >>>>>>>>>>>>>>>>>>>> dim == 2
===========    output   =========== 
 tensor(1.0299, grad_fn=<NllLossBackward>)
```

## 3.2 负对数似然损失—NLLLoss

上面说过了，NLLLoss即pytorch中交叉熵loss的底层实现，NLLLoss(*negative log likelihood )，公式如下：*

 ![[公式]](https://www.zhihu.com/equation?tex=%5Cell%28x%2C+y%29+%3D+L+%3D+%7Bl_1%2C%5Cdots%2Cl_N%7D%5E%5Ctop%2C+%5Cquad+++++++++l_n+%3D+-+w_%7By_n%7D+x_%7Bn%2Cy_n%7D%2C+%5Cquad+++++++++w_%7Bc%7D+%3D+%5Ctext%7Bweight%7D%5Bc%5D+%5Ccdot+%5Cmathbb%7B1%7D%7Bc+%5Cnot%3D+%5Ctext%7Bignore_index%7D%7D%2C) 

 ![[公式]](https://www.zhihu.com/equation?tex=%5Cell%28x%2C+y%29+%3D+%5Cbegin%7Bcases%7D+++++++++++++%5Csum%7Bn%3D1%7D%5EN+%5Cfrac%7B1%7D%7B%5Csum_%7Bn%3D1%7D%5EN+w_%7By_n%7D%7D+l_n%2C+%26+++++++++++++%5Ctext%7Bif+reduction%7D+%3D+%5Ctext%7B%27mean%27%3B%7D%5C+++++++++++++%5Csum_%7Bn%3D1%7D%5EN+l_n%2C++%26+++++++++++++%5Ctext%7Bif+reduction%7D+%3D+%5Ctext%7B%27sum%27.%7D+++++++++%5Cend%7Bcases%7D) 

看代码注释or[官方文档](https://link.zhihu.com/?target=https%3A//pytorch.org/docs/stable/nn.html%23torch.nn.NLLLoss)中的描述：此函数常用在C个类别的分类问题上，且输入参数input为前向传播产生的类别log概率(log-probabilities)。


**input**    可以通过在输出层后加一层logsoftmax层来获得input的log概率，也可以直接使用_CrossEntropyLoss()*来直接处理input。*
***target**   target为真实分类的索引index向量，index范围为：[0,_C*−1] 。譬如5分类任务：dog,cat,pig,horse,fish 则target向量内index的范围为0~4，对于batch_size = 4的一批input，target = [0,1,3,2]表示这一批input的真实分类为dog,cat,horse,pig
 
实际上，NLLLoss通常是用来实现交叉熵函数的，只是输入input需要经过log和softmax处理，典型的应用方式为：**output = F.nll_loss(F.log_softmax(input), target)。**

```python
class NLLLoss(_WeightedLoss):
    __constants__ = ['ignore_index', 'reduction']

    def __init__(self, weight=None, size_average=None, ignore_index=-100,
                 reduce=None, reduction='mean'):
        super(NLLLoss, self).__init__(weight, size_average, reduce, reduction)
        self.ignore_index = ignore_index

    def forward(self, input, target):
        return F.nll_loss(input, target, weight=self.weight, ignore_index=self.ignore_index, reduction=self.reduction)
```

## 3.3 二元交叉熵损失—BCELoss

**二元交叉熵损失BCELoss(Binary Cross Entropy)，实际上其不仅可以用于二分类，还可用于多标签分类问题中**。BCELoss(Binary Cross Entropy)公式：

![[公式]](https://www.zhihu.com/equation?tex=%5Cell%28x%2C+y%29+%3D+L+%3D+%7Bl_1%2C%5Cdots%2Cl_N%7D%5E%5Ctop%2C+%5Cquad+++++++++l_n+%3D+-+w_n+%5Cleft%5B+y_n+%5Ccdot+%5Clog+x_n+%2B+%281+-+y_n%29+%5Ccdot+%5Clog+%281+-+x_n%29+%5Cright%5D%2C) 
 
 ![[公式]](https://www.zhihu.com/equation?tex=%5Cell%28x%2C+y%29+%3D+%5Cbegin%7Bcases%7D+++++++++++++%5Coperatorname%7Bmean%7D%28L%29%2C+%26+%5Ctext%7Bif+reduction%7D+%3D+%5Ctext%7B%27mean%27%3B%7D%5C+++++++++++++%5Coperatorname%7Bsum%7D%28L%29%2C++%26+%5Ctext%7Bif+reduction%7D+%3D+%5Ctext%7B%27sum%27.%7D+++++++++%5Cend%7Bcases%7D) 
 
[文档](https://link.zhihu.com/?target=https%3A//pytorch.org/docs/stable/nn.html%3Fhighlight%3Dbcewithlogitsloss%23torch.nn.BCEWithLogitsLoss)里指出：当x = 0(或1)时公式里有一项是log0,在pytorch中log0 = −∞，故x→0时，log(*x*)=−∞ .为了避免公式失效和无意义的计算，这里限制了输出最小值>=-100，从而保证loss和反向传播的过程可以计算。

还有一点：光看公式，我们可以发现，二元交叉熵是多元交叉熵的特例，实际上二分类也完全可以用之前的CrossEntropyLoss()实现。

```python
class BCELoss(_WeightedLoss):
    __constants__ = ['reduction']

    def __init__(self, weight=None, size_average=None, reduce=None, reduction='mean'):
        super(BCELoss, self).__init__(weight, size_average, reduce, reduction)

    def forward(self, input, target):
        return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)
```

使用BCELoss需要将输入用Sigmoid()函数处理。下面看一个多标签分类的例子：
例：

```python
import torch
from torch import nn

m = nn.Sigmoid()  # 不能用nn.Softmax()
loss = nn.BCELoss()
input = torch.randn(3, requires_grad=True)
target = torch.empty(3).random_(2)
output = loss(m(input), target)
output.backward()
print(input, '\n', target, '\n', output)

# 输出
# tensor([-0.4477,  1.1807,  0.9555], requires_grad=True)
# tensor([0., 0., 1.])
# tensor(0.7560, grad_fn=<BinaryCrossEntropyBackward>)
```


**BCEWithLogitsLoss**

和BCELoss差不多，其差别在于**BCEWithLogitsLoss将Sigmoid层和BCELoss合并在一起，比单独使用Sigmoid + BCELoss处理的方式，其数值更稳定。**因为在BCEWithLogitsLoss的实现中，使用了log-sum-exp的trick。

## 3.4 平均绝对损失—MAELoss/L1 Loss

MAE平均绝对损失(_mean absolute error)，_即通常所说的L1-norm损失/L1损失。公式如下：


 ![[公式]](https://www.zhihu.com/equation?tex=%5Cell%28x%2C+y%29+%3D+L+%3D+%7Bl_1%2C%5Cdots%2Cl_N%7D%5E%5Ctop%2C+%5Cquad+++++++++l_n+%3D+%5Cleft%7C+x_n+-+y_n+%5Cright%7C) 
 ![[公式]](https://www.zhihu.com/equation?tex=%5Cell%28x%2C+y%29+%3D+++++++++%5Cbegin%7Bcases%7D+++++++++++++%5Coperatorname%7Bmean%7D%28L%29%2C+%26+%5Ctext%7Bif+reduction%7D+%3D+%5Ctext%7B%27mean%27%3B%7D%5C+++++++++++++%5Coperatorname%7Bsum%7D%28L%29%2C++%26+%5Ctext%7Bif+reduction%7D+%3D+%5Ctext%7B%27sum%27.%7D+++++++++%5Cend%7Bcases%7D) 


这里x表示输入，y表示输出，N为一个批次里的样本数量batch size，损失即为二者的相减的绝对值。参数reduction默认为mean表示返回的是所有项的平均loss,reduction = sum时返回的是所有项损失的相加和。**MAE损失在深度学习的图像领域一般用的较少，适合简单的回归问题和一些非神经网络的简单模型。**
 
pytorch中的定义——nn.L1Loss()

```python
class L1Loss(_Loss):
    __constants__ = ['reduction']

    def __init__(self, size_average=None, reduce=None, reduction='mean'):
        super(L1Loss, self).__init__(size_average, reduce, reduction)

    def forward(self, input, target):
        return F.l1_loss(input, target, reduction=self.reduction)
```

例子：

```python
import torch
from torch import nn

loss = nn.L1Loss()
input = torch.randn(2, 3, requires_grad=True)
target = torch.randn(2, 3)
output = loss(input, target)
output.backward()
print(input, '\n', target, '\n', output)

# 输出
# tensor([[0.7813, 2.5570, 0.2680],
#     [1.8305, 1.4230, 0.6720]], requires_grad=True) 
# tensor([[-0.6951, -0.0510,  0.8427],
#     [-1.0945, -1.2718, -0.8232]]) 
# tensor(1.9624, grad_fn=<L1LossBackward>)
```

## 3.5 平滑L1损失—SmoothL1Loss

平滑L1损失(smooth l1 loss)是L1损失的改进版本，为什么叫smooth l1？

![img](https://pic2.zhimg.com/v2-2675c28b5450a18cabe44bf199718725_b.jpg)


由图中可以看出，在坐标原点附近，smooth l1 loss转折十分平滑，不像 l1 loss 有个尖角，因此叫做 smooth l1 loss。faster-rcnn中的bbox回归损失就使用了smooth l1 loss。
 
公式如下：
 ![[公式]](https://www.zhihu.com/equation?tex=%5Ctext%7Bloss%7D%28x%2C+y%29+%3D+%5Cfrac%7B1%7D%7Bn%7D+%5Csum_%7Bi%7D+z_%7Bi%7D) 
 ![[公式]](https://www.zhihu.com/equation?tex=z_%7Bi%7D+%3D+++++++++%5Cbegin%7Bcases%7D+++++++++0.5+%28x_i+-+y_i%29%5E2%2C+%26+%5Ctext%7Bif+%7D+%7Cx_i+-+y_i%7C+%3C+1+%5C%5C+++++++++%7Cx_i+-+y_i%7C+-+0.5%2C+%26+%5Ctext%7Botherwise+%7D+++++++++%5Cend%7Bcases%7D) 


pytorch中的定义——nn.SmoothL1Loss()

```python
class SmoothL1Loss(_Loss):
    __constants__ = ['reduction']

    def __init__(self, size_average=None, reduce=None, reduction='mean'):
        super(SmoothL1Loss, self).__init__(size_average, reduce, reduction)

    def forward(self, input, target):
        return F.smooth_l1_loss(input, target, reduction=self.reduction)
```

### 为什么Faster-RCNN中不用L2损失？

对于bbox回归问题，通常也可以选择平方损失函数（L2损失），但L2损失的缺点是当存在离群点（outliers)的时候，这些点会占loss的主要组成部分，**可能带来梯度爆炸的问题；**且平方损失函数对于**训练初期loss较大(导数也较大)，loss下降的太快，不稳定**，所以采用线性的l1损失更好。

### 为什么用smooth l1 loss而不用l1 loss?

这里，设置x为输入值和truth之间的差异，比较二者导数：


 ![[公式]](https://www.zhihu.com/equation?tex=%5Cbegin%7Barray%7D%7Bc%7D+%5Cfrac%7B%5Cmathrm%7Bd%7D+L_%7B1%7D%28x%29%7D%7B%5Cmathrm%7Bd%7D+x%7D%3D%5Cleft%5C%7B%5Cbegin%7Barray%7D%7Bll%7D+1+%26+%5Ctext+%7B+if+%7D+x+%5Cgeq+0+%5C%5C+-1+%26+%5Ctext+%7B+otherwise+%7D+%5Cend%7Barray%7D%5Cright.+%5C%5C+%5Cend%7Barray%7D) 
 
 ![[公式]](https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cmathrm%7Bd%7D+%5Coperatorname%7Bsmooth%7D_%7BL_%7B1%7D%7D%7D%7B%5Cmathrm%7Bd%7D+x%7D%3D%5Cleft%5C%7B%5Cbegin%7Barray%7D%7Bcc%7D+x+%26+%5Ctext+%7B+if+%7D%7Cx%7C%3C1+%5C%5C+%5Cpm+1+%26+%5Ctext+%7B+otherwise+%7D+%5Cend%7Barray%7D%5Cright.) 

设想，当模型训练后期，loss较低准确率较高时，x值会较小，当x趋近0时。l1 loss导数为1或-1，梯度呈线性下降，在学习率lr不变时，线性下降的梯度可能使得loss在最小值附近波动，无法下降到最小值附近；而smooth ls loss的导数为x，且x很小趋于0，梯度下降值也同步较小，能使得模型的训练在后期也能缓慢迭代优化。

>  参考： [请问 faster RCNN 和 SSD 中为什么用smooth L1 loss，和L2有什么区别？](https://www.zhihu.com/question/58200555/answer/621174180)

## 3.6 均方误差损失—MSELoss/L2Loss

MSELoss均方误差损失(*mean squared error*)，即通常所说的L2损失/平方损失。在CNN网络中，应用l2 loss可以使得损失下降的更快(相比l1 loss)。MSE loss公式如下：
 ![[公式]](https://www.zhihu.com/equation?tex=%5Cell%28x%2C+y%29+%3D+L+%3D+%7Bl_1%2C%5Cdots%2Cl_N%7D%5E%5Ctop%2C+%5Cquad+++++++++l_n+%3D+%5Cleft%28+x_n+-+y_n+%5Cright%29%5E2%2C) 
 
 ![[公式]](https://www.zhihu.com/equation?tex=%5Cell%28x%2C+y%29+%3D+++++++++%5Cbegin%7Bcases%7D+++++++++++++%5Coperatorname%7Bmean%7D%28L%29%2C+%26++%5Ctext%7Bif+reduction%7D+%3D+%5Ctext%7B%27mean%27%3B%7D%5C+++++++++++++%5Coperatorname%7Bsum%7D%28L%29%2C++%26++%5Ctext%7Bif+reduction%7D+%3D+%5Ctext%7B%27sum%27.%7D+++++++++%5Cend%7Bcases%7D) 


pytorch中的定义——nn.MSELoss()

```python
class MSELoss(_Loss):
    __constants__ = ['reduction']

    def __init__(self, size_average=None, reduce=None, reduction='mean'):
        super(MSELoss, self).__init__(size_average, reduce, reduction)

    def forward(self, input, target):
        return F.mse_loss(input, target, reduction=self.reduction)
```

例：

```python
import torch
from torch import nn
loss = nn.MSELoss()
input = torch.randn(2, 3, requires_grad=True)
target = torch.randn(2, 3)
output = loss(input, target)
output.backward()
print(input, '\n', target, '\n', output)

# # 输出
# tensor([[ 0.3067, -0.0677, -1.5623],
#         [-0.3898, -0.1272, -1.0483]], requires_grad=True) 
# tensor([[-0.7882,  1.1236, -1.2984],
#         [ 0.0710, -2.2593,  1.3296]]) 
# tensor(2.1834, grad_fn=<MseLossBackward>)
```

**损失函数**用来评价模型的**预测值**和**真实值**不一样的程度，损失函数越好，通常模型的性能越好。不同的模型用的损失函数一般也不一样。

**损失函数**分为**经验风险损失函数**和**结构风险损失函数**。经验风险损失函数指预测结果和实际结果的差别，结构风险损失函数是指经验风险损失函数加上正则项。

# 机器学习常见的损失函数以及其优缺点如下：

## 1.**0-1损失函数(zero-one loss)**

0-1损失是指预测值和目标值不相等为1， 否则为0:

![[公式]](https://www.zhihu.com/equation?tex=L+%28+Y+%2C+f+%28+X+%29+%29+%3D+%5Cleft%5C%7B+%5Cbegin%7Barray%7D+%7B+l+%7D+%7B+1+%2C+Y+%5Cneq+f+%28+X+%29+%7D+%5C%5C+%7B+0+%2C+Y+%3D+f+%28+X+%29+%7D+%5Cend%7Barray%7D+%5Cright.+++%5C%5C)

特点：

(1)0-1损失函数直接对应分类判断错误的个数，但是它是一个非凸函数，不太适用.

(2)**感知机**就是用的这种损失函数。但是相等这个条件太过严格，因此可以放宽条件，即满足 ![[公式]](https://www.zhihu.com/equation?tex=%7CY+-+f%28x%29%7C+%3C+T) 时认为相等，

![[公式]](https://www.zhihu.com/equation?tex=L+%28+Y+%2C+f+%28+X+%29+%29+%3D+%5Cleft%5C%7B+%5Cbegin%7Barray%7D+%7B+l+%7D+%7B+1+%2C+%7C+Y+-+f+%28+X+%29+%7C+%5Cgeq+T+%7D+%5C%5C+%7B+0+%2C+%7C+Y+%3D+f+%28+X+%29+%7C+%3C+T+%7D+%5Cend%7Barray%7D+%5Cright.++%5C%5C)

## **绝对值损失函数**

绝对值损失函数是计算预测值与目标值的差的绝对值：

![[公式]](https://www.zhihu.com/equation?tex=L%28Y%2C+f%28x%29%29+%3D+%7CY+-+f%28x%29%7C++%5C%5C)

## **log对数损失函数**

**log对数损失函数**的标准形式如下：

![[公式]](https://www.zhihu.com/equation?tex=L%28Y%2C+P%28Y%7CX%29%29+%3D+-logP%28Y%7CX%29++%5C%5C)

特点：

(1) log对数损失函数能非常好的表征概率分布，在很多场景尤其是多分类，如果需要知道结果属于每个类别的置信度，那它非常适合。

(2)健壮性不强，相比于hinge loss对噪声更敏感。

(3)**逻辑回归**的损失函数就是log对数损失函数。

## **平方损失函数**

平方损失函数标准形式如下：

![[公式]](https://www.zhihu.com/equation?tex=L+%28+Y+%7C+f+%28+X+%29+%29+%3D+%5Csum+_+%7B+N+%7D+%28+Y+-+f+%28+X+%29+%29+%5E+%7B+2+%7D++%5C%5C)

特点：

(1)经常应用与回归问题

## **指数损失函数（exponential loss）**

**指数损失函数**的标准形式如下：

![[公式]](https://www.zhihu.com/equation?tex=L%28Y%7Cf%28X%29%29+%3D+exp%5B-yf%28x%29%5D++%5C%5C)

特点：

(1)对离群点、噪声非常敏感。经常用在AdaBoost算法中。

## **Hinge 损失函数**

Hinge损失函数标准形式如下：

![[公式]](https://www.zhihu.com/equation?tex=L%28y%2C+f%28x%29%29+%3D+max%280%2C+1-yf%28x%29%29+++%5C%5C)

特点：

(1)hinge损失函数表示如果被分类正确，损失为0，否则损失就为 ![[公式]](https://www.zhihu.com/equation?tex=1-yf%28x%29) 。**SVM**就是使用这个损失函数。

(2)一般的 ![[公式]](https://www.zhihu.com/equation?tex=f%28x%29) 是预测值，在-1到1之间， ![[公式]](https://www.zhihu.com/equation?tex=y) 是目标值(-1或1)。其含义是， ![[公式]](https://www.zhihu.com/equation?tex=f%28x%29+) 的值在-1和+1之间就可以了，并不鼓励 ![[公式]](https://www.zhihu.com/equation?tex=%7Cf%28x%29%7C+%3E+1) ，即并不鼓励分类器过度自信，让某个正确分类的样本距离分割线超过1并不会有任何奖励，从而**使分类器可以更专注于整体的误差。**

(3) 健壮性相对较高，对异常点、噪声不敏感，但它没太好的概率解释。

## **感知损失(perceptron loss)函数**

**感知损失函数**的标准形式如下：

![[公式]](https://www.zhihu.com/equation?tex=L%28y%2C+f%28x%29%29+%3D+max%280%2C+-f%28x%29%29++%5C%5C)

特点：

(1)是Hinge损失函数的一个变种，Hinge loss对判定边界附近的点(正确端)惩罚力度很高。而perceptron loss**只要样本的判定类别正确的话，它就满意，不管其判定边界的距离**。它比Hinge loss简单，因为不是max-margin boundary，所以模**型的泛化能力没 hinge loss强**。

## **交叉熵损失函数 (Cross-entropy loss function)**

**交叉熵损失函数**的标准形式如下**:**

![[公式]](https://www.zhihu.com/equation?tex=C+%3D+-+%5Cfrac+%7B+1+%7D+%7B+n+%7D+%5Csum+_+%7B+x+%7D+%5B+y+%5Cln+a+%2B+%28+1+-+y+%29+%5Cln+%28+1+-+a+%29+%5D++%5C%5C)

注意公式中 ![[公式]](https://www.zhihu.com/equation?tex=x) 表示样本， ![[公式]](https://www.zhihu.com/equation?tex=y) 表示实际的标签， ![[公式]](https://www.zhihu.com/equation?tex=a) 表示预测的输出， ![[公式]](https://www.zhihu.com/equation?tex=n) 表示样本总数量。

特点：

(1)本质上也是一种**对数似然函数**，可用于二分类和多分类任务中。

二分类问题中的loss函数（输入数据是softmax或者sigmoid函数的输出）：

![[公式]](https://www.zhihu.com/equation?tex=loss+%3D+-+%5Cfrac+%7B+1+%7D+%7B+n+%7D+%5Csum+_+%7B+x+%7D+%5B+y+%5Cln+a+%2B+%28+1+-+y+%29+%5Cln+%28+1+-+a+%29+%5D+%5C%5C)

多分类问题中的loss函数（输入数据是softmax或者sigmoid函数的输出）：

![[公式]](https://www.zhihu.com/equation?tex=loss+%3D+-+%5Cfrac%7B1%7D%7Bn%7D+%5Csum_i+y_ilna_i+%5C%5C)

(2)当使用sigmoid作为激活函数的时候，常用**交叉熵损失函数**而不用**均方误差损失函数**，因为它可以**完美解决平方损失函数权重更新过慢**的问题，具有“误差大的时候，权重更新快；误差小的时候，权重更新慢”的良好性质。

最后奉献上交叉熵损失函数的实现代码：[cross_entropy](https://link.zhihu.com/?target=https%3A//github.com/yyHaker/MachineLearning/blob/master/src/common_functions/loss_functions.py).



------

这里需要更正一点，**对数损失函数和交叉熵损失函数应该是等价的！！！**

下面来具体说明：

![img](https://pic4.zhimg.com/80/v2-ac627eab5f07ead5144cfaaff7f2163b_720w.jpg)

------

**相关高频问题：**

1.**交叉熵函数**与**最大似然函数**的联系和区别？

区别：**交叉熵函数**使用来描述模型预测值和真实值的差距大小，越大代表越不相近；**似然函数**的本质就是衡量在某个参数下，整体的估计和真实的情况一样的概率，越大代表越相近。

联系：**交叉熵函数**可以由最大似然函数在伯努利分布的条件下推导出来，或者说**最小化交叉熵函数**的本质就是**对数似然函数的最大化**。

怎么推导的呢？我们具体来看一下。

设一个随机变量 ![[公式]](https://www.zhihu.com/equation?tex=X) 满足伯努利分布，

![[公式]](https://www.zhihu.com/equation?tex=P%28X%3D1%29+%3D+p%2C+P%28X%3D0%29%3D1-p+%5C%5C+)

则 ![[公式]](https://www.zhihu.com/equation?tex=X) 的概率密度函数为：

![[公式]](https://www.zhihu.com/equation?tex=P%28X%29%3Dp%5EX%281-p%29%5E%7B1-X%7D++%5C%5C)

因为我们只有一组采样数据 ![[公式]](https://www.zhihu.com/equation?tex=D) ，我们可以统计得到 ![[公式]](https://www.zhihu.com/equation?tex=X) 和 ![[公式]](https://www.zhihu.com/equation?tex=1-X) 的值，但是 ![[公式]](https://www.zhihu.com/equation?tex=p) 的概率是未知的，接下来我们就用**极大似然估计**的方法来估计这个 ![[公式]](https://www.zhihu.com/equation?tex=p) 值。

对于采样数据 ![[公式]](https://www.zhihu.com/equation?tex=D) ，其**对数似然函数**为:

![[公式]](https://www.zhihu.com/equation?tex=logP%28D%29+%3D+log%5Cprod_%7Bi%7D%5E%7BN%7DP%28D_i%29+%5C%5C+%3D+%5Csum_%7Bi%7Dlogp%28D_i%29+%5C%5C+%3D+%5Csum_%7Bi%7D%28D_ilogp+%2B+%281-D_i%29log%281-p%29%29++%5C%5C)

可以看到上式和**交叉熵函数**的形式几乎相同，**极大似然估计**就是要求这个式子的最大值。而由于上面函数的值总是小于0，一般像神经网络等对于损失函数会用最小化的方法进行优化，所以一般会在前面加一个负号，得到**交叉熵函数**（或**交叉熵损失函数**）：

![[公式]](https://www.zhihu.com/equation?tex=loss+%3D+-%5Csum_%7Bi%7D%28D_ilogp+%2B+%281-D_i%29log%281-p%29%29+++%5C%5C)

这个式子揭示了**交叉熵函数**与**极大似然估计**的联系，**最小化交叉熵函数**的本质就是**对数似然函数的最大化。**

现在我们可以用求导得到极大值点的方法来求其**极大似然估计**，首先将对数似然函数对 ![[公式]](https://www.zhihu.com/equation?tex=p) 进行求导，并令导数为0，得到

![[公式]](https://www.zhihu.com/equation?tex=%5Csum_%7Bi%7D%28D_i+%5Cfrac%7B1%7D%7Bp%7D+%2B+%281-D_i%29+%5Cfrac%7B1%7D%7Bp-1%7D%29+%3D0+%5C%5C)

消去分母，得：

![[公式]](https://www.zhihu.com/equation?tex=%5Csum_%7Bi%7D%5E%7BN%7D%28p-D_i%29+%3D+0+%5C%5C)

所以:

![[公式]](https://www.zhihu.com/equation?tex=p++%3D+%5Cfrac%7B1%7D%7BN%7D+%5Csum_i+D_i++%5C%5C)

这就是伯努利分布下**最大似然估计**求出的概率 ![[公式]](https://www.zhihu.com/equation?tex=p) 。

2.在用sigmoid作为激活函数的时候，为什么要用**交叉熵损失函数**，而不用**均方误差损失函数**？

其实这个问题求个导，分析一下两个误差函数的参数更新过程就会发现原因了。

对于**均方误差损失函数**，常常定义为：

![[公式]](https://www.zhihu.com/equation?tex=C%3D+%5Cfrac%7B1%7D%7B2n%7D%5Csum_x%28a+-+y%29%5E2+%5C%5C)

其中 ![[公式]](https://www.zhihu.com/equation?tex=y) 是我们期望的输出， ![[公式]](https://www.zhihu.com/equation?tex=a) 为神经元的实际输出（ ![[公式]](https://www.zhihu.com/equation?tex=a+%3D+%5Csigma%28z%29%2C+z%3Dwx%2Bb) ）。在训练神经网络的时候我们使用梯度下降的方法来更新 ![[公式]](https://www.zhihu.com/equation?tex=w) 和 ![[公式]](https://www.zhihu.com/equation?tex=b) ，因此需要计算代价函数对 ![[公式]](https://www.zhihu.com/equation?tex=w) 和 ![[公式]](https://www.zhihu.com/equation?tex=b) 的导数：

![[公式]](https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+C+%7D%7B%5Cpartial+w%7D+%3D+%28a-y%29%5Csigma%27%28z%29x+%5C%5C+%5Cfrac%7B%5Cpartial+C+%7D%7B%5Cpartial+b+%7D+%3D+%28a-y%29%5Csigma%27%28z%29%5C%5C)

然后更新参数 ![[公式]](https://www.zhihu.com/equation?tex=w) 和 ![[公式]](https://www.zhihu.com/equation?tex=b) ：

![[公式]](https://www.zhihu.com/equation?tex=w+%3D+w+-+%5Ceta+%5Cfrac%7B%5Cpartial+C%7D%7B%5Cpartial+w%7D+%3D+w+-+%5Ceta+%28a-y%29%5Csigma%27%28z%29x+%5C%5C+b+%3D+b+-+%5Ceta+%5Cfrac%7B%5Cpartial+C%7D%7B%5Cpartial+b%7D+%3D+b+-+%5Ceta+%28a-y%29%5Csigma%27%28z%29+%5C%5C)

因为sigmoid的性质，导致 ![[公式]](https://www.zhihu.com/equation?tex=%5Csigma%27%28x%29) 在 ![[公式]](https://www.zhihu.com/equation?tex=z) 取大部分值时会很小（如下图标出来的两端，几乎接近于平坦），这样会使得 ![[公式]](https://www.zhihu.com/equation?tex=%5Ceta+%28a-y%29%5Csigma%27%28z%29) 很小，导致参数 ![[公式]](https://www.zhihu.com/equation?tex=w) 和 ![[公式]](https://www.zhihu.com/equation?tex=b) 更新非常慢。

![img](https://pic1.zhimg.com/80/v2-c5c4f0aaab1c73b82caca103ef6d8050_720w.jpg)

那么为什么**交叉熵损失函数**就会比较好了呢？同样的对于**交叉熵损失函数**，计算一下参数更新的梯度公式就会发现原因。**交叉熵损失函数**一般定义为：

![[公式]](https://www.zhihu.com/equation?tex=C+%3D+-+%5Cfrac+%7B+1+%7D+%7B+n+%7D+%5Csum+_+%7B+x+%7D+%5B+y+%5Cln+a+%2B+%28+1+-+y+%29+%5Cln+%28+1+-+a+%29+%5D+%5C%5C)

其中 ![[公式]](https://www.zhihu.com/equation?tex=y) 是我们期望的输出， ![[公式]](https://www.zhihu.com/equation?tex=a) 为神经元的实际输出（ ![[公式]](https://www.zhihu.com/equation?tex=a+%3D+%5Csigma%28z%29%2C+z%3Dwx%2Bb) ）。同样可以看看它的导数：

![[公式]](https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+C%7D%7B%5Cpartial+a%7D+%3D+-%5Cfrac%7B1%7D%7Bn%7D%5Csum_x%5By%5Cfrac%7B1%7D%7Ba%7D%2B%28y-1%29%5Cfrac%7B1%7D%7B1-a%7D%5D+%5C%5C+%3D+-%5Cfrac%7B1%7D%7Bn%7D%5Csum_x%5B%5Cfrac%7B1%7D%7Ba%281-a%29%7Dy+-+%5Cfrac%7B1%7D%7B1-a%7D%5D+%5C%5C+%3D+-%5Cfrac%7B1%7D%7Bn%7D%5Csum_x%5B%5Cfrac%7B1%7D%7B%5Csigma%28x%29%281-%5Csigma%28x%29%29%7Dy+-+%5Cfrac%7B1%7D%7B1-%5Csigma%28x%29%7D%5D+%5C%5C)

另外，

![[公式]](https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+C%7D%7B%5Cpartial+z%7D+%3D+%5Cfrac%7B%5Cpartial+C%7D%7B%5Cpartial+a%7D+%5Cfrac%7B%5Cpartial+a%7D%7B%5Cpartial+z%7D+%5C%5C+%3D-%5Cfrac%7B1%7D%7Bn%7D%5Csum_x%5B%5Cfrac%7B1%7D%7B%5Csigma%28x%29%281-%5Csigma%28x%29%29%7Dy+-+%5Cfrac%7B1%7D%7B1-%5Csigma%28x%29%7D%5D+%5Cbullet+%5Csigma%27%28x%29+%5C%5C+%3D+-%5Cfrac%7B1%7D%7Bn%7D%5Csum_x%5B%5Cfrac%7B1%7D%7B%5Csigma%28x%29%281-%5Csigma%28x%29%29%7Dy+-+%5Cfrac%7B1%7D%7B1-%5Csigma%28x%29%7D%5D%5Cbullet+%5Csigma%28x%29%281-%5Csigma%28x%29%29+%5C%5C+%3D+-%5Cfrac%7B1%7D%7Bn%7D+%5Csum_x%28y-a%29+%5C%5C)

所以有：

![[公式]](https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+C%7D%7B%5Cpartial+w%7D+%3D+%5Cfrac%7B%5Cpartial+C%7D%7B%5Cpartial+z%7D+%5Cfrac%7B%5Cpartial+z%7D%7B%5Cpartial+w%7D+%3D+%28a-y%29x++%5C%5C)

![[公式]](https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+C%7D%7B%5Cpartial+b%7D+%3D+%5Cfrac%7B%5Cpartial+C%7D%7B%5Cpartial+z%7D+%5Cfrac%7B%5Cpartial+z%7D%7B%5Cpartial+b%7D+%3D+%28a-y%29++%5C%5C)

所以参数更新公式为：

![[公式]](https://www.zhihu.com/equation?tex=w+%3D+w+-+%5Ceta+%5Cfrac%7B%5Cpartial+C%7D%7B%5Cpartial+w%7D+%3D+w+-+%5Ceta+%28a-y%29x%5C%5C+b+%3D+b+-+%5Ceta+%5Cfrac%7B%5Cpartial+C%7D%7B%5Cpartial+b%7D+%3D+b+-+%5Ceta+%28a-y%29+%5C%5C)

可以看到参数更新公式中没有 ![[公式]](https://www.zhihu.com/equation?tex=%5Csigma%27%28x%29) 这一项，权重的更新受 ![[公式]](https://www.zhihu.com/equation?tex=%28a-y%29) 影响，受到误差的影响，所以***当误差大的时候，权重更新快；当误差小的时候，权重更新慢\***。这是一个很好的性质。



所以当使用sigmoid作为激活函数的时候，常用**交叉熵损失函数**而不用**均方误差损失函数**