import numpy as np

#get_max_iou
def get_max_iou(prebox, gtbox):
    if prebox.shape[0] > 0:
        ixmin = np.maximum(prebox[:, 0], gtbox[0])
        iymin = np.maximum(prebox[:, 1], gtbox[1])
        ixmax = np.minimum(prebox[:, 2], gtbox[2])
        iymax = np.minimum(prebox[:, 3], gtbox[3])

        iw = np.maximum(ixmax - ixmin + 1, 0)
        ih = np.maximum(iymax - iymin + 1, 0)

        inters = iw*ih
        union = ((gtbox[2] - gtbox[0] + 1)*(gtbox[3] - gtbox[1] + 1) +
                 (prebox[:, 2] - prebox[:, 0] + 1)*(prebox[:, 3] - prebox[:, 1] + 1) - inters)

        iou = inters/union
        maxiou = np.max(iou)
        idx = np.argmax(iou)

    return maxiou, idx


#nms:
def nms(dets, thresh):
    x1 = dets[:, 0]
    y1 = dets[:, 1]
    x2 = dets[:, 2]
    y2 = dets[:, 3]
    score = dets[:, 4]
    area = (y2 - y1 + 1)*(x2 - x1 + 1)
    order = score.argsort()[::-1]
    keep = []
    while order.size > 0:
        i = order[0]
        keep.append(i)
        xx1 = np.maximum(x1[i], x1[order[1:]])
        yy1 = np.maximum(y1[i], y1[order[1:]])
        xx2 = np.minimum(x2[i], x2[order[1:]])
        yy2 = np.minimum(y2[i], y2[order[1:]])
        w = np.maximum(0.0, xx2 - xx1 + 1)
        h = np.maximum(0.0, yy2 - yy1 + 1)
        inter = w*h
        iou = inter/(area[i] + area[order[1:]] - inter)
        idx = np.where(iou<=thresh)[0]
        order = order[idx+1]
    return keep

#soft_nms:

#softmax:
def softmax(x, axis = 1):
    row_max = x.max(axis = axis)
    row_max = row_max.reshape(-1, 1)
    x = x - row_max

    x_exp = np.exp(x)
    x_sum = np.sum(x_exp, axis = axis, keepdims=True)
    s = x_exp/x_sum
    return s

#多分类交叉熵
def CEloss(y, y_hat):
    assert y.shape == y_hat.shape
    n = 1e-6
    res = -np.sum(np.nan_to_num(y*np.log(y_hat+n)), axis=1)
    return res
    #return np.mean(res)


#ROC曲线下的面积为Auc，ROC为TPR(N为正样本个数)-FPR(N负样本个数)曲线，反映模型在选取不同阈值的时候其敏感性（sensitivity, FPR）和其精确性（specificity, TPR）的趋势走向
def AUC(label, pre):
    # 计算正样本和负样本的索引，以便索引出之后的概率值
    pos = [i for i in range(len(label)) if label[i] == 1]
    neg = [i for i in range(len(label)) if label[i] == 0]

    auc = 0
    for i in pos:
        for j in neg:
            if pre[i] > pre[j]:
                auc += 1
            elif pre[i] == pre[j]:
                auc += 0.5

    return auc / (len(pos) * len(neg))


import torch
#sigmoid函数
def sigmoid(X):
    return 1.0/(1.0 + np.exp(-X))
#relu函数
def fun_relu(X):
    X = np.where(X>=0,X,0)
    return torch.tensor(X)

#im2col, conv, pooling
def my_img2col(img, filter, padding=1, strides=1):
    row, col = img.shape[:2]
    tp_row, tp_col = filter.shape
    img_new = np.zeros((row + 2*padding, col + 2*padding))
    img_new[padding:row+padding, padding:padding+col] = img
    out_h, out_w = 1 + (row+2*padding-tp_row)//strides, 1 + (col+2*padding-tp_col)//strides
    im2col = np.zeros((tp_row * tp_col, out_h * out_w))

    count = 0
    for y in range(0, out_w):
        for x in range(0, out_h):
            cur_item = img_new[y*strides: y*strides + tp_row, x*strides: x*strides + tp_col].reshape(-1)
            im2col[:, count] = cur_item
            count += 1
    return im2col, (out_h, out_w)

def myconv(img, template, padding=1, strides=1):
    im2col, transfer_shape = my_img2col(img, template, padding=padding, strides=strides)
    filter_flatten = template.reshape(1, -1)
    conv_out = np.matmul(filter_flatten, im2col)
    return conv_out.reshape(transfer_shape[0], transfer_shape[1])

def mypooling(img, template, padding=1, strides=1):
    im2col, transfer_shape = my_img2col(img, template, padding=padding, strides=strides)
    filter_flatten = template.reshape(1, -1)
    pool_out = np.mean(im2col, axis=0)
    #pool_out = np.mean(im2col, axis=0)
    return pool_out.reshape(transfer_shape[0], transfer_shape[1])



#KNN
import collections
def KNN_classify(k,X_train,y_train,x):
  """
    k:表示knn的中k的值
    X_train: 训练集的features
    y_train: 训练集的labels
    x: 新的数据
  """
  assert 1<=k<=X_train.shape[0],"k must be valid"
  assert X_train.shape[0] == y_train.shape[0], \
  "the size of X_train must equal to the size of y_train"
  assert X_train.shape[1] == x.shape[0], \
  "the feature number of x must to be equal to X_train"
  # 计算新来的数据x与整个训练数据中每个样本数据的距离
  distances = [np.sqrt(np.sum((x_train-x)**2)) for x_train in X_train]
  nearest = np.argsort(distances) # 对距离排序并返回对应的索引

  topK_y = [y_train[i] for i in nearest][:k] # 返回最近的k个距离对应的分类
  votes = collections.Counter(topK_y) # 统计属于每个分类的样本数

  return votes.most_common(1)[0][0] # 返回属于样本数最多的分类结果


#Kmeans
class kmeans:
    def __init__(self):
        pass
    def distEclud(self, x, y):
        return np.sqrt(np.sum((x - y)**2))

    def randCenter(self, dataset, k):
        m, n = dataset.shape
        centerids = np.zeros((k, n))
        for i in range(k):
            idx = int(np.random.uniform(0, m))
            centerids[i, :] = dataset[idx, :]
        return centerids

    def km(self, dataset, k):
        m , _= dataset.shape
        clusterAssment = np.zeros((m, 2))
        clusterChange = True

        centerids = self.randCenter(dataset, k)
        while clusterChange:
            clusterAssment = False

        for i in range(m):
            mindist = 10000
            minidx = -1
            for j in range(k):
                dist = self.distEclud(centerids[j, :], dataset[i, :])
                if dist < mindist:
                    mindist = dist
                    minidx = j
            if clusterAssment[i, 0] != minidx:
                clusterChange = True
                clusterAssment[i, :] = minidx, mindist

        for i in range(k):




import numpy as np
import math


            def double_linear(input_signal, zoom_multiples):
                '''
                双线性插值
                :param input_signal: 输入图像
                :param zoom_multiples: 放大倍数
                :return: 双线性插值后的图像
                '''
                input_signal_cp = np.copy(input_signal)  # 输入图像的副本

                input_row, input_col = input_signal_cp.shape  # 输入图像的尺寸（行、列）

                # 输出图像的尺寸
                output_row = int(input_row * zoom_multiples)
                output_col = int(input_col * zoom_multiples)

                output_signal = np.zeros((output_row, output_col))  # 输出图片

                for i in range(output_row):
                    for j in range(output_col):
                        # 输出图片中坐标 （i，j）对应至输入图片中的最近的四个点点（x1，y1）（x2, y2），（x3， y3），(x4，y4)的均值
                        temp_x = i / output_row * input_row
                        temp_y = j / output_col * input_col

                        x1 = int(temp_x)
                        y1 = int(temp_y)

                        x2 = x1
                        y2 = y1 + 1

                        x3 = x1 + 1
                        y3 = y1

                        x4 = x1 + 1
                        y4 = y1 + 1

                        u = temp_x - x1
                        v = temp_y - y1

                        # 防止越界
                        if x4 >= input_row:
                            x4 = input_row - 1
                            x2 = x4
                            x1 = x4 - 1
                            x3 = x4 - 1
                        if y4 >= input_col:
                            y4 = input_col - 1
                            y3 = y4
                            y1 = y4 - 1
                            y2 = y4 - 1

                        # 插值
                        output_signal[i, j] = (1 - u) * (1 - v) * int(input_signal_cp[x1, y1]) + (1 - u) * v * int(
                            input_signal_cp[x2, y2]) + u * (1 - v) * int(input_signal_cp[x3, y3]) + u * v * int(
                            input_signal_cp[x4, y4])
                return output_signal


def Batchnorm_simple_for_train(x, gamma, beta, bn_param, momentum):
    """
    param:x    : 输入数据，设shape(B,L)
    param:gama : 缩放因子  γ
    param:beta : 平移因子  β
    param:bn_param   : batchnorm所需要的一些参数
        eps      : 接近0的数，防止分母出现0
        momentum : 动量参数，一般为0.9， 0.99， 0.999
        running_mean ：滑动平均的方式计算新的均值，训练时计算，为测试数据做准备
        running_var  : 滑动平均的方式计算新的方差，训练时计算，为测试数据做准备
    """


    running_mean = bn_param['running_mean']  # shape = [B]
    running_var = bn_param['running_var']  # shape = [B]
    results = 0.  # 建立一个新的变量

    x_mean = x.mean(axis=0)  # 计算x的均值
    x_var = x.var(axis=0)  # 计算方差
    x_normalized = (x - x_mean) / np.sqrt(x_var + eps)  # 归一化
    results = gamma * x_normalized + beta  # 缩放平移

    running_mean = momentum * running_mean + (1 - momentum) * x_mean
    running_var = momentum * running_var + (1 - momentum) * x_var

    # 记录新的值
    bn_param['running_mean'] = running_mean
    bn_param['running_var'] = running_var

    return results, bn_param










































